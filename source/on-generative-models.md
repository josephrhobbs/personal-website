# Generative Models

~

::byline[Joseph Hobbs][February 20, 2026]

The current decade (as of this writing) has seen a massive explosion in the common usage of large language models (LLMs) and other generative models for text, audio, images, and video.  Though research in generative models for human language has been ongoing since the latter half of the 20th century, a number of factors contributed to this advent of widespread and everyday usage.  Despite the notable power and accuracy of contemporary language models, their application can be as damaging as it can be useful.  It is rather regrettable to observe that many people are using these models to "ghostwrite" online content, academic literature, formal applications or letters, and other valuable written content.  The use of generative models spread like wildfire in the early 2020s, and nobody can expect to see them go away anytime soon, which means _now is the time_ for society to agree upon what it means to use this powerful technology responsibly.  Here, I have presented briefly my position on this, and to explain my decision to refrain from using generative models for this website, as well as all other online and print content I work on.  I have seen many take a similar position, and therefore it is my ardent hope that people from around the world are beginning to agree on what it means to use generative models responsibly.

It is important to note that, throughout this essay, I will use the term __generative model__ to speak specifically of models for _language_, rather than images or other data.

## A Brief History of Generative Models

The late 1900s and early 2000s saw most attempts to model human language take the form of __recurrent neural networks__ (RNNs).  Recurrent neural networks are effective at modeling sequential data; after all, what is a sentence but a sequence of words?  However, researchers struggled to build RNNs that generalized well to diverse topics and vocabulary.  As researc continued, researchers discovered a new method called __attention__ that enhanced the performance of RNNs.  Attention allows models to build internal representations of words' relevance to one another; for example, in the phrase "the deep blue sea", the words _the_, _deep_, and _blue_ are said to __attend__ to the word _sea_ because they each modify the word _sea_.

In 2017, the publication of ["Attention Is All You Need"](https://arxiv.org/pdf/1706.03762), by A. Vaswani, et al., introduced the use of the Transformer for modeling language.  As the title of the paper suggested, the research team at Google discovered that attention was so good at modeling language that the RNN could be disposed of entirely.  Furthermore, by disposing of recurrent networks (which are inherently sequential) the Transformer architecture was extremely parallelizable, allowing researchers to train models of unprecedented scale.  Though the Transformer was first used for _machine translation_ (translating one language to another), it was quickly discovered that transformers could be used for _generation_ of language (like in a chatbot).

The AI research startup OpenAI began developing a class of Transformer models for language called __Generative Pre-trained Transformers__ (GPTs) in the late 2010s.  In 2022, OpenAI constructed the first Transformer-based chatbot available to consumers, naming it __ChatGPT__.  The release of ChatGPT shook Western culture, and suddenly every major technology company needed their own competing Transformer-based language model.  Google released __Gemini__ as well as the open-source __Gemma__ models; Apple released __Apple Intelligence__; billionaire E. Musk's technology empire released __Grok__; Microsoft released __Copilot__; Amazon released __Rufus__.  Anthropic, a direct competitor to OpenAI, released their suite of __Claude__ products in the early 2020s also, to compete with ChatGPT.  

## Usage of Generative Models

One extremely common use of generative models is what I will call __agentic web search__.  Since the advent of large-scale search engines in the late 1990s, the predominant method for searching the Internet has been typing keywords into a search engine such as Google.  Though search engines have improved over time, they were fundamentally limited because of their inability to interpret search results meaningfully; rather, they simply return a list of web pages that the user may find useful.  Generative models can be very useful for search; instead of only returning a list of hyperlinks, they can also read pages and highlight the specific information in them that the user is looking for.  The user can then ask follow-up questions in a conversational manner, allowing the user to find the desired information very quickly.

However, another very common use of generative models is what I will call __ghostwriting__.  Generative models can generate hundreds of words in seconds, far faster than any human author, and their spelling and grammar is often impeccable.  This has motivated many to begin to use their output in substitute for writing; social media posts and other Internet content, academic literature and peer review, and formal documents such as resumes can be generated easily using these models.  Use of generative models for these purposes, particularly when the resulting content is represented as coming from a human author, is deeply concerning.  Though it often passes as benign, ghostwriting has severe consequences on quality of written work, our own human dignity, and the trajectory of our society's information systems.

### Hallucination and Accountability

A major concern surrounding the use of language models is __hallucination__.  It is commonly known that generative models can create false or misleading answers, particularly when they are unable to answer the question correctly.  The 2025 paper ["Why Language Models Hallucinate"](https://arxiv.org/pdf/2509.04664) by A. Kalai et al. analyzes this phenomenon in almost exhausting detail.  The main theorem of their paper, which I will omit here for brevity, places a lower bound on the hallucination rate of a generative model.  Critically, it is nearly impossible to drive this lower bound to zero, meaning that language model hallucination is not easily repaired, even with techniques like Retrieval-Augmented Generation (RAG) and Chain-of-Thought reasoning (CoT).

When generative models hallucinate during ghostwriting, the incorrect or misleading language become integrated into social media posts, formal publications, or other material.  This accelerates the spread of misinformation in the (already disorienting) Internet information environment.  Furthermore, authors who choose to ghostwrite often have little knowledge or control over these hallucinations, allowing them to spread quickly.  Of course humans can also spread misinformation (knowingly or unknowingly), but a rigorous author who writes authentically and checks his or her facts before publishing is more likely to catch errors than one who copypastes outputs from a language model.  Widespread ghostwriting leads to degradation of accountability in an information ecosystem.

### Decreased Human Neural Connectivity

Concerns of hallucination aside, the use of generative models for ghostwriting has been shown to have serious effects on cognitive engagement during writing.  The MIT Media Lab conducted a study in 2025 titled ["Your Brain on ChatGPT"](https://arxiv.org/pdf/2506.08872), finding that the use of generative models in essay writing caused significant cognitive debt.  The study grouped human participants into the _Brain-only_, _Search Engine_, and _LLM_ groups, and asked all participants to write an essay on a provided topic.  The Brain-only participants were permitted only to use a word processor; the Search Engine participants could use a word processor and a search engine; and the LLM participants could use a word processor, a search engine, and a large language model.

The researchers discovered, using electroencephalography (EEG), that the Brain-only participants showed the strongest neural connectivity, highest engagement of particular neural circuits, and maximal ability to quote from their own essays.  The Search Engine participants demonstrated moderate connectivity, engagement, and quotation ability, and the LLM participatnts performed worst in these three metrics.  This caused the researchers to conclude that using a generative model for writing results in _cognitive debt_... the human brain uses the model as a "crutch", shortcutting typical pathways for deep reasoning and visual processing.  When language is so essential to our humanity, and writing as a consequence is such a deeply human activity, why should we deprive ourselves of the enrichment, understanding, and reasoning that writing affords?

### Model Autophagy Disorder

The final concern of using generative models for ghostwriting is a practical one.  Generative models require massive amounts of data to train properly, and all contemporary models primarily use text found on the Internet for training.  In our increasingly digital world, nearly all publicly available writing can be found on the Internet, including book manuscripts, academic publications, and shorter-form content like essays and news articles.  Every time a ghostwritten work is published on the Internet, our training sets become increasingly "polluted" with synthetic content.

Researchers at Rice University published ["Self-Consuming Generative Models Go MAD"](https://arxiv.org/pdf/2307.01850) in 2023.  Recalling the epidemic of _mad cow disease_ in a recent decade caused by livestock cannibalism, the researchers propose the concept of __Model Autophagy Disorder__ (MAD).  They discovered that training a generative model on synthetic data, generating more synthetic data, and training again caused output quality (relative to real data) to decrease rapidly.  This degradation is analogous to repeatedly photocopying an image, printing the photocopy, then photocopying that photocopy, and so on _ad nauseum_... after a number of iterations, the image loses all of its original detail and becomes hardly recognizeable.  MAD can be alleviated if enough "fresh" real data is included on each iteration, however.  In the case of the Internet, as the amount of ghostwritten content grows, human-written content must grow also to ensure that MAD does not set into widely available consumer language models.  But because ghostwriting is so much faster and cognitively easier than human writing, it is unlikely that widespread ghostwriting will be sustainable and not cause MADness in ChatGPT, Gemini, etc.  Ghostwriting has the effect of "poisoning the well" and negatively affecting the future quality of these models.  In other words, if we ghostwrite now, then nobody will be able to later, nor will anybody be able to enjoy the many beneficial uses of these generative models.

## Looking Forward

Though I regret the prevalence of ghostwriting using generative models is taking its toll, I do not intend this to be a "doomsday" prediction; in fact, quite the opposite!  Extensive research is beginning to uncover the many benefits and potential negative effects of using generative models, and I have seen many people and organizations state well-reasoned opinions on the responsible use of these models.

Looking into the future, as these models will become perhaps only more prevalent, I have a sincere hope that we will continue to mature as a society, to understand and to agree upon what it means to use this new technology responsibly.  Though I am confident that my factual claims are correct, I do not assert the objectivity of my ethical positions.  What is most important is that each of us consider what we know to be _right_, not what we find to be _convenient_, and that we engage one another in respectful and informed dialogue.  And your attention here, reader, is a great comfort to me... there is yet one more person who is not afraid to take that first step in facing the pressing issues of our time.  Thank you.
