<!DOCTYPE html>
<html>

<head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-C43FKL8BR7"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-C43FKL8BR7');
</script>


<title>Hardness Of Nonlinear Programming | Joseph R. Hobbs</title>

<link rel="stylesheet" href="/style.css">

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&family=Outfit:wght@100..900&family=Fira+Code:wght@300..700&display=swap" rel="stylesheet">
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<link href="/prism.css" rel="stylesheet" />
<script src="/prism.js"></script>


<link rel="icon" type="image/x-icon" href="/media/favicon.ico">

</head>

<body>

<h1>Hardness of NLP</h1>

<div class="menu">

<a href="/">Home</a>

<a href="/about">About</a>

<a href="/projects">Projects</a>

<a href="/articles">Articles</a>

</div>



<div class="byline"><div>Joseph Hobbs</div><div>February 10, 2026</div></div>

<p>In recent years I've taken up a great interest in the field of <strong>optimization</strong>.  The study of optimization formalizes an extraordinarily large class of problems that appear <em>literally all the time</em> in mathematics, computer science, engineering, and the natural sciences.  Whenever we are presented with a collection of possible conclusions in natural science, design parameters in engineering, model parameters in statistics and machine learning, or other <strong>decision variables</strong>, we are asked to choose variables based on some <strong>objective</strong>.  For example, there are many, many possible bridges that are safe for their rated capacity, but some are more expensive to build, maintain, and repair than others.  The study of optimization helps us to formalize which choices we are making and why.</p>

<p>Unfortunately for us, the computer science department has a habit of reminding us that optimization is hard.  Like, really hard.  When computer scientists use the word "hard" to describe a problem, they usually don't just mean "difficult" in a subjective sense; they usually mean that the problem lives in a class of problems called <strong>NP-hard</strong>.  Problems in NP-hard are the "most difficult" problems (by a very specific definition) that we know about.  And optimization is, in general, one of those problems.</p>

<p>But not all hope is lost.  Optimization is NP-hard <em>in general</em>, but there are specific cases of optimization which are not!  In fact, many special cases of optimization live in a different class of problems called <strong>P</strong>, which are the "easiest" problems (by a very specific definition) that we can solve!  One class of optimization problems that lies (almost) entirely within P is <strong>convex optimization</strong> problems.  In a future article, I will discuss in detail the topic of convexity and why we care so much about it.</p>

<h2>Review of Optimization</h2>

<p>For those unacquainted, the study of optimization formalizes "optimization problems" as <strong>optimization programs</strong>.  The field makes formal distinctions between various programs and provides rigorous, certifiable methods for solving them.  The most general optimization program is the <strong>Nonlinear Program</strong> (NLP).</p>

<p>\[ \begin{align*} P: \min_x f(x) & \\ \text{subject to } g_i(x) &\le 0 \\ h_j(x) &= 0 \end{align*} \]</p>

<p>Here, the optimization program \( P \) is general because we make <em>no assumptions</em> about the behavior of \( f \), \( g \), or \( h \).  These functions could be extraordinarily "nice" (linear functions, maybe!) or terribly behaved functions.  We call the vector \( x \) the <strong>decision variable</strong>, the scalar function \( f \) the <strong>objective</strong>, and the inequalities and equalities below the <strong>constraints</strong>.</p>

<p>Optimization programs, in general, can have three possible results: <strong>infeasibility</strong>, <strong>unboundedness</strong>, and <strong>optimality</strong>.  As you might imagine, we like that last one the best!  A program is <em>infeasible</em> if there exists no \( x \) that can satisfy the constraints.  For example, consider \( P_1 \).</p>

<p>\[ \begin{align*} P_1: \min_x x^2 & \\ \text{subject to } x &\ge 1 \\ x &\le -1 . \end{align*} \]</p>

<p>Clearly, because there is no such \( x \) such that \( x \ge 1 \) and \( x \le -1 \), this program is <em>infeasible</em>.  A problem is <em>unbounded</em> if there is a feasible region, but there is no optimal solution within it.  This is easily visualized by \( P_2 \).</p>

<p>\[ \begin{align*} P_2: \min_x \frac{1}{x} & \\ \text{subject to } x &\ge 1 . \end{align*} \]</p>

<p>By increasing \( x \), we continually decrease the objective function, but we can never reach a minimum value, because we can just keep increasing \( x \) forever.  The program is feasible, but it is <em>unbounded</em>.  Finally, programs like \( P_3 \) can be solved to optimality.</p>

<p>\[ \begin{align*} P_3: \min_x x^2 & \\ \text{subject to } x &\le 1 . \end{align*} \]</p>

<p>Here, it's clear to see that \( x = 0 \) is the <em>minimizer</em> of \( P_3 \), and the program is solved to optimality.</p>

<h2>Hardness of NLP</h2>

<p>Generally, the problem of solving NLPs is, unfortunately, <strong>NP-hard</strong>.</p>

<p>Hardness of NLP.  <strong>Theorem 1</strong>.  Let \( P \) be an NLP.  Then, solving \( P \) is NP-hard.</p>

<h3>Definition of NP-hardness</h3>

<p>Before I continue, I want to define NP-hardness explicitly.  Informally, a problem is NP-hard if any problem in NP can be converted into the problem in question.</p>

<p>Hardness.  <strong>Definition 1</strong>.  A problem \( Q \) is <em>NP-hard</em> if, for every problem \( R \) in NP, there exists a polynomial-time reduction \( L \) such that \( L(R) \) rewrites \( R \) as \( Q \).</p>

<h3>Lemmas</h3>

<p>I also have two lemmas to present, which will help me significantly in proving Theorem 1.</p>

<p>Hardness of SAT.  <strong>Lemma 1</strong>.  The problem of Boolean satisfiability (SAT) is NP-hard in the number of decision variables.  The problem of Boolean satisfiability is parameterized by decision variables \( x_i \) for \( i = 1, \cdots, N \) and an expression in conjunctive normal form (CNF), involving a finite number of terms joined by AND, where each term is a finite number of terms joined by OR.  For example, the following is CNF. </p>

<p>\[ ( x_1 \lor x_3 \lor x_7 ) \land ( x_2 \lor \neg x_1 ) \land \cdots \]</p>

<p>Polynomial-time reduction of SAT.  <strong>Lemma 2 and Proof</strong>.  SAT can be reduced to an NLP by introducing constraint</p>

<p>\[ x_i (1 - x_i) = 0 \]</p>

<p>for each \( x_i \) and introducing constraint</p>

<p>\[ \sum_{i = 1}^n x_i \ge 1 \]</p>

<p>for each CNF clause.  The first constraint enforces \( x_i \in \{ 0, 1 \} \), where \( 0 \) is falsy and \( 1 \) is truthy.  The second constraint ensures that each CNF clause is truthy; if at least one \( x_i \) in a clause is equal to one (truthy), the sum of \( x_i \) will be at least one and the entire clause will be truthy.  For clauses containing negatives like \( \neg x_1 \), we can replace \( x_i \) with \( 1 - x_i \) in the summation.  If all clauses are truthy, the entire CNF is truthy.  Here, we are only looking for feasible solutions, so we do not require any particular objective function.  The final NLP is</p>

<p>\[ \begin{align*} P_\mathrm{SAT}: \min_x 0 & \\ \text{subject to } x_i (1 - x_i) &= 0 \\ \sum_{i = 1}^n x_i &\ge 1 \end{align*} . \]</p>

<p>Note that we have abused notation to omit cases of negatives like \( \neg x_1 \).  In these cases, we modify the corresponding constraint as previously described.  With \( N \) decision variables, \( k \) CNF clauses, and a maximum of \( n \) decision variables per CNF clause, it is clear that <em>SAT can be reduced to NLP</em> in \( O(N + kn) \) time.</p>

<h3>Proof</h3>

<p>We can now prove Theorem 1!</p>

<p>Hardness of NLP.  <strong>Proof of Theorem 1</strong>.  By Lemma 2, any SAT problem can be reduced to an NLP in \( O(N + kn) \) time.  Therefore, a polynomial-time reduction \( L_1 \) exists from SAT to NLP.  By Lemma 1, SAT is NP-hard, which means there exists a polynomial-time reduction \( L_2 \) exists from any problem in NP to SAT.  Therefore, the reduction \( L_3 := L_1(L_2(\cdot)) \) is polynomial-time.  This means that \( L_3 \) is a polynomial-time reduction from any problem in NP to NLP.  By Definition 1, the existence of \( L_3 \) implies NLP is NP-hard.  \( \blacksquare \)</p></body>

</html>