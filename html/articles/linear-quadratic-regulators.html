<!DOCTYPE html>
<html>

<head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-C43FKL8BR7"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-C43FKL8BR7');
</script>


<title>Linear Quadratic Regulators | Joseph R. Hobbs</title>

<link rel="stylesheet" href="/style.css">

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&family=Outfit:wght@100..900&family=Fira+Code:wght@300..700&display=swap" rel="stylesheet">
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<link href="/prism.css" rel="stylesheet" />
<script src="/prism.js"></script>


<link rel="icon" type="image/x-icon" href="/media/favicon.ico">

</head>

<body>

<h1>LQR</h1>

<div class="menu">

<a href="/">Home</a>

<a href="/about">About</a>

<a href="/projects">Projects</a>

<a href="/articles">Articles</a>

</div>



<div class="byline"><div>Joseph Hobbs</div><div>February 9, 2026</div></div>

<p>The world is full of complicated systems changing over time, interacting with one another, with themselves, and with their environment.  It's difficult to comprehend just how ubiquitous these "systems" are, in the most general sense.  An incredible (and underappreciated) achievement of contemporary engineering is the study of <strong>control</strong>.  Control theorists seek to understand how both natural and engineered systems evolve, and how they can be controlled.</p>

<p>More traditional methods of control, often called <em>classical</em> control, focus on describing the dynamics of a <strong>plant</strong> (the system to be controlled), deciding on the <em>desired</em> dynamics, and then working backwards to determine a good <strong>controller</strong> (an engineered system capable of changing the plant's dynamics).  For example, a classic toy example is the inverted pendulum.  Pendulums do not naturally balance upside-down (existing dynamics), but we want them to (desired dynamics).  So by sending the right torque commands to a motor at the pendulum's pivot, we can make the pendulum balance upside-down.</p>

<p>In recent years, classical control has given way to new ways of thinking about control.  Classical control has one major challenge: how do we determine what dynamics we want in the first place?  The problem becomes rather difficult in the presence of constraints like motor torque limits or minimizing chance of the physical collision of two objects.  For example, consider the case of the pendulum, but we pay a hefty price for every kilojoule of electrical energy expended by the motor.  How do we know what dynamics we "want", apart from anything that's stable and cheap?  Contemporary control theorists have begun reframing the problem of control as an <em>optimization</em>.  These methods of <em>optimal</em> control focus on describing a <strong>cost</strong> (penalty for "bad" outcomes) and then determining controllers that minimize that cost.</p>

<h2>Linear Systems</h2>

<p>For this article, I'm restricting myself to <strong>linear systems</strong>, obeying</p>

<p>\[ \dot{x} = A x + B u \]</p>

<p>for system state \( x \in \mathbb{R}^d \), control input \( u \in \mathbb{R}^k \), and constant matrices \( A \in \mathbb{R}^{d \times d}, B \in \mathbb{R}^{d \times k} \).  Very few systems in our world are truly linear (rotation immediately introduces trigonometric functions, which are obviously not linear functions), but many systems can be considered "close" to linear in certain cases, so this is after all a useful representation.</p>

<h2>Cost</h2>

<p>Notice that, for this linear system, \( x = 0, u = 0 \) is a <em>fixed point</em>.  That means that, when \( x = 0 \) and \( u = 0 \), then \( \dot{x} = 0 \) and the system doesn't change.  This doesn't mean that the system is <em>stable</em> around \( x = 0 \)... if you <em>perfectly</em> balance a pendulum upside-down, it will stay, but as soon as you bump it at all then it will fall.  Suppose we want to make the system stable around the fixed point \( x = 0 \), and we'd like to know what control input \( u \) to use.  One very simple to define cost is to use a <strong>quadratic cost</strong>.  At a given time, the quadratic cost function</p>

<p>\[ g(x, u) = x^\mathrm{T} Q x + u^\mathrm{T} R u \]</p>

<p>quantifies how "bad" we are doing.  For \( Q \succeq 0, R \succeq 0 \), notice that \( g(0, 0) = 0 \) is the global minimum of \( g \).  This means that, when \( x \neq 0 \) or \( u \neq 0 \), the cost is strictly positive, meaning the system is a state less than ideal.  Because \( g \) only tells us about <em>instantaneous</em> cost, we can integrate it to determine the total cost accrued over time.</p>

<p>\[ \begin{align*} J &= \int_0^\infty g(x, u) \, \mathrm{d}t \\ &= \int_0^\infty \left( x^\mathrm{T} Q x + u^\mathrm{T} R u \right) \, \mathrm{d}t \end{align*} \]</p>

<p>Without proof, I will assert that, when \( u = -K x \) for some constant matrix \( K \), this reduces to</p>

<p>\[ J(x) = x^\mathrm{T} S x \]</p>

<p>for some matrix \( S \succeq 0 \).  From now on, I'll call \( g(x, u) \) the <em>cost</em> and \( J(x) \) the <em>cost-to-go</em>.</p>

<div class="notice">More coming soon!</div>

</body>

</html>